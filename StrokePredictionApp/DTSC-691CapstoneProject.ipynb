{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebfab1c",
   "metadata": {},
   "source": [
    "## DTSC - 691 Capstone Project - Stroke Risk Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ec7c9",
   "metadata": {},
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df78093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import the dataset \n",
    "health_data = pd.read_csv(\"StrockDataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94a8da",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f2ac6",
   "metadata": {},
   "source": [
    "#### Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44321b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of the data\n",
    "health_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display first 10 rows of the data\n",
    "health_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471cb5a",
   "metadata": {},
   "source": [
    "#### Class Distribution of Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747fc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['gender','ever_married','work_type','Residence_type','smoking_status']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} distribution:\")\n",
    "    print(health_data[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe84d25",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate summary statistics of the data\n",
    "health_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313f02a",
   "metadata": {},
   "source": [
    "By observing the summary statistics, I found that the minimum age is 0.08, which is not a realistic value. Similarly, the maximum BMI is 97, which is unusually high and should be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20757c1b",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0deb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the missing values in each column\n",
    "health_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00139bd",
   "metadata": {},
   "source": [
    "I  have 201 missing values for BMI . We would handle the missing values by median imputation that would fill in the missing values with the median of BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the median of BMI\n",
    "health_data['bmi'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54610874",
   "metadata": {},
   "source": [
    "I use scikit-learn tool Simple Imputer to fill in missing values with the median 28.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import sci-kit learn  and Simple Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Imputers\n",
    "bmi_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "#Apply Imputator to fill in the missing values with median of BMI\n",
    "health_data['bmi'] = bmi_imputer.fit_transform(health_data[['bmi']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d6aa8",
   "metadata": {},
   "source": [
    "Verify that the dataset contains no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c86f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f12155",
   "metadata": {},
   "source": [
    "Previously, in our summary statistics, we observed that BMI had a maximum value of 97 and age had a minimum value of 0.08, both of which are unrealistic. I need to address these discrepancies. Values above 60 are very rare and likely data errors or extreme outliers. So capping BMI at 60 is a reasonable data cleaning choice to handle extreme values, even if it’s not a “healthy” or “approved” medical cutoff therefore I will cap the maximum BMI value at 60 by replacing all BMI values greater than 60 with 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e341974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the BMI values by giving 60 as maximum value\n",
    "health_data.loc[health_data['bmi'] > 60,'bmi'] = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72f2c4",
   "metadata": {},
   "source": [
    "I will remove rows where the age is less than 1. Since the age column is currently of float datatype, I will convert it to an integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove age below 1\n",
    "health_data = health_data[health_data['age']>= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dbaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert age into integer type\n",
    "health_data['age'] = health_data['age'].round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671426ab",
   "metadata": {},
   "source": [
    "I will also remove the id column because it will not be relevant for my analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d130ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop id column\n",
    "health_data = health_data.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe24c5a",
   "metadata": {},
   "source": [
    "Let's observe summary statistics again after cleaning. I can see that we dont have id column anymore ,  minimum age is  1, and the maximum BMI has been capped at 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747400e6",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86f4f0",
   "metadata": {},
   "source": [
    "##### Box Plot of Age Grouped by Stroke Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a box plot to visualize age distribution among individuals who had stroke and who didnt\n",
    "sns.boxplot(x='stroke', y='age', data=health_data)\n",
    "plt.title('Age Grouped by Stroke Occurence')\n",
    "plt.xlabel('Stroke (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210ebad",
   "metadata": {},
   "source": [
    "The Above plot interprets that those with stroke , ages are generally higher , mostly concentrated between approximately 60 and 80 years, indicating that stroke patients tend to be older"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1abdfa",
   "metadata": {},
   "source": [
    "##### Stacked Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ae0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out unknown category\n",
    "filtered_health_data = health_data[health_data['smoking_status'] != 'Unknown']\n",
    "\n",
    "# Calculate proportions\n",
    "smoking_stroke = pd.crosstab(filtered_health_data['smoking_status'], filtered_health_data['stroke'], normalize='index')\n",
    "\n",
    "# Generate a Stacked Bar chart reperesenting Smoking Status based on individuals who had stroke and not \n",
    "ax = smoking_stroke.plot(kind='bar', stacked=True, colormap='coolwarm', figsize=(8, 5))\n",
    "\n",
    "# Add percentage labels\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(p.get_x() + p.get_width()/2,\n",
    "                p.get_y() + height/2,\n",
    "                f'{height*100:.1f}%',\n",
    "                ha='center', va='center', fontsize=15, color='black')\n",
    "\n",
    "# Add Labels and title to the plot\n",
    "plt.title('Proportion of Stroke Occurence within Smoking Status Groups')\n",
    "plt.xlabel('Smoking Status')\n",
    "plt.ylabel('Proportion')\n",
    "plt.legend(title='Stroke', labels=['No', 'Yes'])\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a14f7",
   "metadata": {},
   "source": [
    "Due to highly imbalanced stroke class we receive a higher proportion of non-stroke cases compared to stroke. From the plot i found that people who are formerly smoked have more chance of experiencing a stroke followed by current smokers and those never smoked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db96736",
   "metadata": {},
   "source": [
    "##### KDE Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a KDE plot to visualize average glucose levels among individuals who had stroke and those who did not\n",
    "sns.kdeplot(health_data[health_data['stroke'] == 0]['avg_glucose_level'], label='No Stroke', fill=True)\n",
    "sns.kdeplot(health_data[health_data['stroke'] == 1]['avg_glucose_level'], label='Stroke', fill=True)\n",
    "plt.title('Glucose Level Distribution Among Stroke and Non-Stroke Individuals')\n",
    "plt.xlabel('Average Glucose Level')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da1cf3",
   "metadata": {},
   "source": [
    "The plot compares the distribution of average glucose levels between individuals who had a stroke (orange) and those who did not (blue). We see a higher and narrow peak around 100 mg/dL which indicates large number of people are under normal range. A secondary peak around 200 mg/dL, suggesting a notable subgroup with significantly elevated glucose levels, possibly linked to diabetes or hyperglycemia. This distribution indicates that both normal and high glucose levels are observed in stroke patients, but higher glucose levels are more prevalent among them than in non-stroke individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c688ff0",
   "metadata": {},
   "source": [
    "#### Correlation HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c47c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the metrics of features that are related to stroke \n",
    "corr = health_data.corr(numeric_only=True)\n",
    "\n",
    "#Generate a correlation heatmap \n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr,annot=True,cmap='coolwarm',fmt='.2f',linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b2c76",
   "metadata": {},
   "source": [
    "Age has a correlation of 0.25 with stroke, indicating a moderate positive relationship. This means that as age increases, the likelihood of having a stroke tends to increase slightly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a724a5",
   "metadata": {},
   "source": [
    "##### Balance the Stroke Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a0d82",
   "metadata": {},
   "source": [
    "Lets check the class distribution of the Stroke column which is our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6bc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class distribution of stroke column\n",
    "health_data['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3190f84",
   "metadata": {},
   "source": [
    "I observes a high class imbalance in stroke cases, with the majority being non-stroke. This imbalance can lead to poor model performance, so it is important to apply techniques to balance the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a48b4d",
   "metadata": {},
   "source": [
    "To Address the stroke class imbalance we use CTGAN(Conditional Tabular Generative Adversarial Network) a deep learning model that learns underlying distributions of data and generates synthetic samples that mimics the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea311033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdv\n",
    "from sdv.metadata import Metadata\n",
    "import pandas as pd\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Set seeds for reproducibility . Usually all random generators are called.\n",
    "SEED = 42\n",
    "# for Python in-built random-generators \n",
    "random.seed(SEED)\n",
    "# sets the seed for numpy and pandas random generator , \n",
    "np.random.seed(SEED)\n",
    "# CTGAN primarily uses pytorch random generator because it is built on top of pytorch\n",
    "torch.manual_seed(SEED) \n",
    "\n",
    "#Create a copy of health_data based on positive stroke cases\n",
    "stroke_positive = health_data[health_data['stroke'] == 1].copy() \n",
    "\n",
    "#Figures out the type of each column (e.g., categorical, numerical, boolean) in our dataset\n",
    "metadata = Metadata.detect_from_dataframe(data=stroke_positive)\n",
    "\n",
    "synthesizer = CTGANSynthesizer(metadata)\n",
    "synthesizer.fit(stroke_positive)\n",
    "\n",
    "# Generate synthetic samples \n",
    "synthetic_data = synthesizer.sample(num_rows=2409) # 50% of non-stroke cases\n",
    "synthetic_data['stroke'] = 1  # Add target label back\n",
    "\n",
    "# Combine with original dataset\n",
    "health_data_balanced = pd.concat([health_data, synthetic_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55e948",
   "metadata": {},
   "source": [
    "The model may take some time to run based on the OS. Verify the stroke class again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc248f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data_balanced['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae1e4f",
   "metadata": {},
   "source": [
    "#### Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da97e9",
   "metadata": {},
   "source": [
    "Split our balanced dataset for training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = health_data_balanced.drop('stroke',axis=1)\n",
    "target = health_data_balanced['stroke']\n",
    "\n",
    "x_train_val,x_test,y_train_val,y_test = train_test_split(features,target,test_size=0.2,random_state=42,stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60d6d0",
   "metadata": {},
   "source": [
    "Since our hypertension and heart_disease columns have only values between 0 and 1 we dont need to standardize the data .Encode categorical features using one-hot encoding and standardize numerical features using StandardScaler for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c730bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#create a variable called numerical_cols to extract numerical columns from the data\n",
    "numerical_cols = ['age','avg_glucose_level','bmi']\n",
    "#create a variable called numerical_cols to extract columns with binary values  from the data\n",
    "binary_cols = ['hypertension','heart_disease']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('cat',OneHotEncoder(handle_unknown='ignore'),categorical_cols),\n",
    "        ('num',StandardScaler(),numerical_cols),\n",
    "        ('bin','passthrough',binary_cols)\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#Fit and tranform on training data and transform only on testing data\n",
    "x_train_encoded = preprocessor.fit_transform(x_train_val)\n",
    "x_test_encoded = preprocessor.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7d430",
   "metadata": {},
   "source": [
    "I will be training and evaluating 7 different models and find the best performing model that would predict stroke risk . A good recall ,accuracy  and better precision would be focus in our model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203dcfd",
   "metadata": {},
   "source": [
    "#### ML Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353d97c",
   "metadata": {},
   "source": [
    "#### Random Forest : Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c962964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_f = RandomForestClassifier(n_estimators=100,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569bf7f",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import  make_scorer,recall_score\n",
    "\n",
    "recall_scorer= make_scorer(recall_score,average='binary')\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [17],\n",
    "    'min_samples_split': [10],\n",
    "}\n",
    "\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=random_f,\n",
    "    param_grid=rf_param_grid,       \n",
    "    scoring=recall_scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "rf_model = rf_grid_search.fit(x_train_encoded,y_train_val)\n",
    "\n",
    "print(\"Best parameters:\", rf_grid_search.best_params_)\n",
    "print(\"Best recall score:\", rf_grid_search.best_score_)\n",
    "\n",
    "rf_best_model = rf_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27790e5",
   "metadata": {},
   "source": [
    "#### Random Forest: Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rand_proba = rf_best_model.predict_proba(x_test_encoded)[:, 1]\n",
    "y_rand_custom = (y_rand_proba >= 0.5).astype(int)\n",
    "y_rand_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd07bc",
   "metadata": {},
   "source": [
    "#### Random Forest : Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da689928",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_test, y_rand_custom)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Random Forest - Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcac2ca",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a72ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_rand_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035eaeee",
   "metadata": {},
   "source": [
    "##### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3eaae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "#Calculate ROC curve and AUC\n",
    "fpr, tpr , thresholds = roc_curve(y_test,y_rand_custom)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "#plot ROC-AUC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr,tpr,label=f'ROC curve(AUC= {roc_auc:.2f})',linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44c329",
   "metadata": {},
   "source": [
    "##### Gradient Boost: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33836d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gboost_model = GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c3774",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'learning_rate': [0.05,0.1],\n",
    "    'max_depth': [5],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "gb_search = GridSearchCV(\n",
    "    estimator=gboost_model,\n",
    "    param_grid= gb_param_grid,\n",
    "    scoring=recall_scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "\n",
    ")\n",
    "\n",
    "gb_model = gb_search.fit(x_train_encoded,y_train_val)\n",
    "\n",
    "print(\"Best parameters:\",gb_search.best_params_)\n",
    "print(\"Best recall score:\", gb_search.best_score_)\n",
    "\n",
    "gb_best_model = gb_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904f7b12",
   "metadata": {},
   "source": [
    "##### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfa4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gb_proba = gb_best_model.predict_proba(x_test_encoded)[:, 1]\n",
    "y_gb_custom = (y_gb_proba >= 0.5).astype(int)\n",
    "y_gb_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ec662",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4eac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_gb_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d899c",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e76441",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_gb_custom)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Gradient Boost - Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46615bfb",
   "metadata": {},
   "source": [
    "##### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ROC curve and AUC\n",
    "fpr, tpr , thresholds = roc_curve(y_test,y_gb_custom)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "#plot ROC-AUC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr,tpr,label=f'ROC curve(AUC= {roc_auc:.2f})',linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776a662",
   "metadata": {},
   "source": [
    "##### Ada Boost : Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define base estimator (optional)\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Define AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier(estimator=base_estimator,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482831bd",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf22a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameter grid for AdaBoost\n",
    "ada_param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'learning_rate': [0.01,0.05, 0.1],\n",
    "    'estimator__max_depth': [5]  # tuning the base learner\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Grid Search\n",
    "ada_search = GridSearchCV(\n",
    "    estimator=adaboost_model,\n",
    "    param_grid=ada_param_grid,\n",
    "    scoring=recall_scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "ada_model = ada_search.fit(x_train_encoded, y_train_val)\n",
    "\n",
    "# Results\n",
    "print(\"Best parameters:\", ada_search.best_params_)\n",
    "print(\"Best Recall score:\", ada_search.best_score_)\n",
    "\n",
    "# Best model\n",
    "ada_best_model = ada_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b6ac5",
   "metadata": {},
   "source": [
    "##### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a546259",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ab_proba = ada_best_model.predict_proba(x_test_encoded)[:, 1]\n",
    "y_ab_custom = (y_ab_proba >= 0.5).astype(int)\n",
    "y_ab_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf08236",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f33978",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_ab_custom)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"AdaBoost - Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51645c9d",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_ab_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a56225b",
   "metadata": {},
   "source": [
    "##### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ROC curve and AUC\n",
    "fpr, tpr , thresholds = roc_curve(y_test,y_ab_custom)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "#plot ROC-AUC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr,tpr,label=f'ROC curve(AUC= {roc_auc:.2f})',linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22261ce",
   "metadata": {},
   "source": [
    "##### Classification Neural Network Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "#Define the model\n",
    "NN_model = Sequential([\n",
    "    Dense(100,activation='relu',input_shape=(x_train_encoded.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(60,activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(60,activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "NN_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e94848",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = NN_model.fit(x_train_encoded,y_train_val,epochs=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d80bd",
   "metadata": {},
   "source": [
    "Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af676a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = NN_model.predict(x_test_encoded)\n",
    "neural_predict = (y_pred_prob >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930aa07e",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, neural_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32778acd",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b22784",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, neural_predict)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"NeuralNetwork - Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e4380",
   "metadata": {},
   "source": [
    "##### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ROC curve and AUC\n",
    "fpr, tpr , thresholds = roc_curve(y_test,neural_predict)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "#plot ROC-AUC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr,tpr,label=f'ROC curve(AUC= {roc_auc:.2f})',linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b4f8f",
   "metadata": {},
   "source": [
    "##### Logistic Regression : Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed17814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000,class_weight='balanced',random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2d044",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54865a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'] \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring=recall_scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    "\n",
    ")\n",
    "\n",
    "lg_model = grid_search.fit(x_train_encoded,y_train_val)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best Recall score:\", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b417ee4",
   "metadata": {},
   "source": [
    "##### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = best_model.predict_proba(x_test_encoded)[:, 1]\n",
    "y_pred_custom = (y_proba >= 0.5).astype(int)\n",
    "y_pred_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f25283",
   "metadata": {},
   "source": [
    "##### Logistic Regression: Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17636b",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6975f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_custom)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Logistic Regression- Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0cfb0",
   "metadata": {},
   "source": [
    "##### Classification Report including Accuracy ,Precision , Recall and F1 Score of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50629abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794defd",
   "metadata": {},
   "source": [
    "##### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "#Calculate ROC curve and AUC\n",
    "fpr, tpr , thresholds = roc_curve(y_test,y_pred_custom)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "#plot ROC-AUC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr,tpr,label=f'ROC curve(AUC= {roc_auc:.2f})',linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01486fff",
   "metadata": {},
   "source": [
    "#### Graph Neural Network Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff599ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "feature_cols = ['age','hypertension','heart_disease','avg_glucose_level']\n",
    "graph_features = health_data_balanced[feature_cols].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "graph_features = scaler.fit_transform(graph_features)\n",
    "\n",
    "target_var = health_data_balanced['stroke'].values\n",
    "\n",
    "x = torch.tensor(graph_features,dtype=torch.float)\n",
    "y = torch.tensor(target_var,dtype=torch.long)\n",
    "\n",
    "#Create edges using KNN\n",
    "k = 3 \n",
    "nbrs = NearestNeighbors(n_neighbors=k+1).fit(graph_features) \n",
    "_, indices = nbrs.kneighbors(graph_features) \n",
    "\n",
    "\n",
    "#Build edge list \n",
    "edge_sources = []\n",
    "edge_targets = []\n",
    "\n",
    "\n",
    "for i, neighbors in enumerate(indices):\n",
    "    for j in neighbors[1:]: \n",
    "        edge_sources.append(i) \n",
    "        edge_targets.append(j)\n",
    "\n",
    "edge_index = torch.tensor([edge_sources,edge_targets],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc02e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b836c17",
   "metadata": {},
   "source": [
    "##### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b73405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class StrokeGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32):\n",
    "        super(StrokeGNN,self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim,hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim,hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim,2)\n",
    "\n",
    "    def forward(self,data):\n",
    "        x,edge_index = data.x,data.edge_index\n",
    "\n",
    "        x = self.conv1(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x,edge_index)\n",
    "        x= F.relu(x)\n",
    "\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1152cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "data = Data(x=x,edge_index=edge_index,y=y)\n",
    "\n",
    "# Split manually using sklearn\n",
    "train_mask, test_mask = train_test_split(torch.arange(len(y)), test_size=0.2, stratify=y)\n",
    "\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "data.train_mask[train_mask] = True\n",
    "data.test_mask[test_mask] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ac5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gnn_model = StrokeGNN(input_dim=x.size(1)).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)\n",
    "\n",
    "# Compute weights based on training labels only\n",
    "# Extract training labels\n",
    "train_labels = data.y[data.train_mask].cpu().numpy()\n",
    "\n",
    "# Convert classes to NumPy array\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "# Compute weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
    "\n",
    "# Convert to tensor and move to device\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define weighted loss\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "def train():\n",
    "    gnn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = gnn_model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    gnn_model.eval()\n",
    "    out = gnn_model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "    correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "    acc = int(correct.sum()) / int(data.test_mask.sum())\n",
    "    return acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | Test Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb19ae",
   "metadata": {},
   "source": [
    "Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e165c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model.eval()\n",
    "\n",
    "# Forward pass\n",
    "out = gnn_model(data)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probs = torch.softmax(out, dim=1)\n",
    "\n",
    "# Predicted class labels\n",
    "preds = probs.argmax(dim=1)\n",
    "\n",
    "# Filter for test nodes\n",
    "y_true = data.y[data.test_mask].cpu().numpy()\n",
    "y_pred_gnn = preds[data.test_mask].cpu().numpy()\n",
    "\n",
    "y_pred_gnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8e07f",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred_gnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecac734",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred_gnn)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Graph Neural Network - Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258330e1",
   "metadata": {},
   "source": [
    "##### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0eb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ROC curve and AUC\n",
    "fpr, tpr , thresholds = roc_curve(y_true,y_pred_gnn)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "#plot ROC-AUC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr,tpr,label=f'ROC curve(AUC= {roc_auc:.2f})',linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7d544",
   "metadata": {},
   "source": [
    "##### XGBoost: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier( eval_metric='aucpr', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364e5b2",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\n",
    "    'max_depth': [6],\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [300],\n",
    "    'scale_pos_weight': [2]\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid, scoring='recall', cv=3, verbose=2)\n",
    "xgb_grid_search.fit(x_train_encoded, y_train_val)\n",
    "\n",
    "print(\"Best params:\", xgb_grid_search.best_params_)\n",
    "print(\"Best recall:\", xgb_grid_search.best_score_)\n",
    "\n",
    "xgb_best_model = xgb_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca08db",
   "metadata": {},
   "source": [
    "##### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7717e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_proba = xgb_best_model.predict_proba(x_test_encoded)[:, 1]\n",
    "xgb_custom = (xgb_proba >= 0.5).astype(int)\n",
    "xgb_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49cedf2",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324317f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, xgb_custom)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"XGBoost - Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d419c",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, xgb_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc8806",
   "metadata": {},
   "source": [
    "##### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d839097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ROC curve and AUC\n",
    "fpr, tpr , thresholds = roc_curve(y_test,xgb_custom)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "#plot ROC-AUC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr,tpr,label=f'ROC curve(AUC= {roc_auc:.2f})',linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')  # Diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a4a3a",
   "metadata": {},
   "source": [
    "#### SHAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e391344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# Get the fitted OneHotEncoder and StandardScaler\n",
    "ohe = preprocessor.named_transformers_['cat']\n",
    "scaler = preprocessor.named_transformers_['num']\n",
    "\n",
    "# Get encoded categorical feature names\n",
    "encoded_cat_features = ohe.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Combine with numerical column names\n",
    "all_feature_names = np.concatenate([encoded_cat_features, numerical_cols])\n",
    "\n",
    "passthrough_cols = [col for col in x_train_val.columns if col not in categorical_cols + numerical_cols]\n",
    "all_feature_names = np.concatenate([encoded_cat_features, numerical_cols, passthrough_cols])\n",
    "\n",
    "explainer = shap.Explainer(xgb_best_model,x_test_encoded)\n",
    "shap_values = explainer(x_test_encoded,check_additivity=False)\n",
    "\n",
    "shap.summary_plot(shap_values, x_test_encoded,feature_names=all_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c630c64",
   "metadata": {},
   "source": [
    "To deploy the model to streamlit , i will use a model pipeline that would preprocess data and fit the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02206812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor',preprocessor),\n",
    "    ('classifier',xgb_best_model)\n",
    "])\n",
    "\n",
    "model_pipeline.fit(x_train_val,y_train_val)\n",
    "\n",
    "print(model_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2c6b6",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for scikitlearn\n",
    "import joblib\n",
    "joblib.dump(model_pipeline,\"stroke_model_sklearn.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
